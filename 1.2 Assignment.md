# Assignment 1.2
# Problem Statement
To Make a CNN Model.
 # Requirment to Run The code
Google Colab (Other compilers can also be use who support ipynb file. But tensorflow, keras, numpy is to be install on the system.

# Libraries Used

> from keras.datasets import mnist

To import MNIST Dataset from keras
> from sklearn.metrics import classification_report

To generate classification report for getting F-Scores

>from sklearn.metrics import confusion_matrix 

To generate confusion matrix

> from keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, MaxPool2D, Flatten  
from keras.models import Model

To create CNN Model

# Structure of Code
 Reshapping is similarly done as in 1.1 Assignment. only differnce is we add add 1 extra dimension because cnn layer can use 3D inputs. The reason for adding 1 is beacuse we are dealing with grey scale image in case of color images we will use 3
 > x_train=x_train.reshape(60000, 28, 28, 1)   
Here 28, and 28 are height pixel and width pixel repectively and 1 is extra dimension.

Same to_categorical function is used for y_train, and y_test as in assignment 1.1.

For converting the (0,255) pixel values into (0,1) similar process is used as in assignment 1.1.


 # Model

> model = Sequential 

sequential model is imported

> Conv2D(filters=32, kernel_size=(7,7), strides=(1), input_shape=(28, 28,1))

In this model we used:
 - 32 filters
 - (7,7) Kernel_size
 - (1) Stride
 - (28,28,1) input shape
    
> Activation("relu")
Relu activation function is used

> BatchNormalization(momentum=0.1)
Btchnormalization with momentum = 0.1

> MaxPool2D

With pool size of (2,2) and Strides=(1)

> Flatten ()
Flattening is converting the data into a dimensional array for inputting it to the next layer.

> Dense(units=1024), Activation("relu"), BatchNormalization(momentum=0.1),

Dense layer with 1024 units, Relu activation is applied, momemtum for batch normalization is 0.1


> model.summary()

For the summary of model


> opt = keras.optimizers.Adam()                
model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["accuracy"])

Adam optimizer is used with categorical_Crossentropy loss.

> history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.2)

history function is created and in this fucnction model.fit is used to train the model with batch size=128, and validation split = 0.2. the reason for creating history function is to plot the graph. 

> preds = model.predict(x_test, batch_Size=128)         
preds = preds.argmax(axis-1) to get the predicted number in the form of (0 to 9)
Y_test = Y_test.argmax(axis=1) to return into normal format (0 t0 9).

For predication

> print(classification_report(y_test, preds))     
>  confusion_matrix(y_test, preds)

Used for fscore and confusion matrix.