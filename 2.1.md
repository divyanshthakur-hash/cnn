# Assignment 2.1 and 2.2
# Problem Statement 2.1
To make a CNN Model from scratch to classify the images of the line datasets into 96 classes.


# Requirment to Run The code
- Google Colab (Other compilers can also be use who support ipynb file. But tensorflow, keras, numpy is to be install on the system.
- output_line.zip file in gdrive

# Structure of Code

> from google.colab import drive  
drive.mount('/content/drive')

To mount gdrive with google colab to access the dataset files.

> !unzip /content/drive/My\ Drive/output_line.zip

This command is used to unzip the folder containing training and valdiateion folder (Spliiting of data set is done by jupyter notebook in local drive of laptop. output_line.zip folder contains train and val folder with 96 classes each)

> PATH = '/content/output_line/'  
train_dir = os.path.join(PATH, 'train')  
val_dir = os.path.join(PATH, 'val')

assigned two different paths for training and validation directory.


> train_0_dir = os.path.join(train_dir, '0_0_0_0')

for testing

> num_0_tr = len(os.listdir(train_0_dir))

for testing

> print(num_0_tr)

for checking no. of images into the folder

> batch_size = 500   

> train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data

to rescale into (0,1) from (0,255)

> train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,                                                           directory=train_dir,
target_size=(IMG_HEIGHT, IMG_WIDTH),
class_mode='categorical')

This function returns tf.data.Dataet that yields batches of images from subdirectories of 96 classes. together with labels of 0 to 95 for train folder.

> val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size, directory=val_dir, shuffle=False, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode='categorical')

this function returns tf.data.Dataet that yields batches of images from subdirectories of 96 classes. together with labels of 0 to 95 for val folder

# Model

> model = Sequential 

Sequential model is imported

# In this model following hyperparameters are used:

> Conv2D(filters=32, kernel_size=(7,7), strides=(1), input_shape=(28, 28,1))

 - 32 filters
 - (7,7) Kernel_size
 - (1) Stride
 - (28,28,3) input shape
    
> Activation("relu")

 Relu activation function is used

> BatchNormalization(momentum=0.1)

  Btchnormalization with momentum = 0.1
 
> MaxPool2D

With pool size of (2,2) and Strides=(1)


> Flatten ()

Flattening is converting the data into a dimensional array for inputting it to the next layer.

> Dense(units=1024), Activation("relu"), BatchNormalization(momentum=0.1),

Dense layer with 1024 units, Relu activation is applied, momemtum for batch normalization is 0.1

> model.summary()

For the summary of model


> opt = keras.optimizers.Adam()                
model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["accuracy"])

Adam optimizer is used with categorical_Crossentropy loss.

> loss, acc = model.evaluate_generator(val_data_gen )

for getting loss and accuracy.

> from sklearn.metrics import confusion_matrix
confusion_matrix(val_data_gen.classes[val_data_gen.index_array],y_pred)

for confusion matrix

> from sklearn.metrics import classification_report
print(classification_report(val_data_gen.classes[val_data_gen.index_array], y_pred))

for Fscores 

**THE RESULTS AND SCREEENSHOT OF THE MODEL IS ATTACHED IN REPORT FILE**

# Problem Statement 2.2

To create own network architecture

Requirement to run the code and structure of code is similar to 2.1.

The only difference is in the model

# Model

# Model

> model = Sequential 

Sequential model is imported

# In this model following hyperparameters are used:

> Conv2D(filters=32, kernel_size=(7,7), strides=(1), input_shape=(28, 28,1))

 - 32 filters
 - (7,7) Kernel_size
 - (1) Stride
 - (28,28,3) input shape
    
> Activation("relu")

 Relu activation function is used

> BatchNormalization(momentum=0.1)

  Btchnormalization with momentum = 0.1
 
> MaxPool2D

With pool size of (2,2) and Strides=(1)

> Conv2D(filters=64, kernel_size=(7,7), strides=(1), input_shape=(28, 28,1))

 - 64 filters
 - (7,7) Kernel_size
 - (1) Stride
 - (28,28,3) input shape
    
> Activation("relu")

 Relu activation function is used

> BatchNormalization(momentum=0.1)

  Btchnormalization with momentum = 0.1
 
> MaxPool2D

With pool size of (2,2) and Strides=(1)


> Flatten ()

Flattening is converting the data into a dimensional array for inputting it to the next layer.

> Dense(units=1024), Activation("relu"), BatchNormalization(momentum=0.1),

Dense layer with 1024 units, Relu activation is applied, momemtum for batch normalization is 0.1

> model.summary()

For the summary of model

The rest of the part is similar to 2.1.

**THE RESULTS AND SCREENSHOT OF MODEL IS ATTACHED IN THE REPORT**