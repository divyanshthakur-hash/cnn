# Assignment 1.2
## Problem Statement
To Make a CNN Model.

# Requirement for Execution of Code
- Python (Programming language)
- Google Colab (Compiler)
- tensorflow ()
- keras ()
- numpy ()
- matplotlib.pyplot ()

# MNIST Dataset
The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples.
> **To import mnist data**  
>>from keras.datasets import mnist

> **To load mnist data into the code**  
>>(x_train, y_train), (x_test, y_test) = mnist.load_data() 

> **To reshape the image by adding channel for training set (1 channel for grey scale, 3 channels for color image)**
>>x_train=x_train.reshape(60000, 28, 28, 1)

> **To reshape the image by adding channel for test set (1 channel for grey scale, 3 channels for color image)**
>>x_train=x_train.reshape(10000, 28, 28, 1)

> **To convert pixel values from (0,255) to (0,1)**
>> x_train.min() = 0, x_train.max()=255
>>>x_train= x_train / x_test.max()
>>>>x_test = x_test / x_test.max()

> **One Hot Encode for Predicting one class from ten classes**
>> - from keras.utils import to_categorical
>> - y_train = to_categorical(y_train, num_classes=10)
>> - y_test = to_categorical(y_test, num_classes=10)
>> - y_train.shape, y_test.shape 
>>>  y_train[]  Becomes 60000 rows with vector of size 10

# Model

For creating CNN Model. I import following libraries:
  - Input
  - Dense
  - Conv2d
  - BtachNormalization
  - Activation
  - MaxPool2D
  - Flatte
  - Model

> from keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, MaxPool2D, Flatten

>from keras.models import Model

**Hyper-Parameters Used**

- Filters=32
> Conv2D(filters=32)
- Kernel_Size=(7,7)
> Conv2D(kernel_size=(7,7))
- Strides = (1,1)
> Conv2D(strides=(1,1))
- Activation = relu
> Conv2D(Activation("relu"))
- Pool Size=(2,2)
> MaxPool2D(pool_size=(2,2))
- Dense = 1024
> Dense(units=1024)
- Dense = 10
> Dense(units=10)
- optimizer = Adam
> opt = keras.optimizers.SGD()
>> model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["accuracy"])
- Btach Size = 128

# Variation tried

In the designed model. I used two different optimizers to attain best accuracy and loss.

## SGD
By using SGD Optimizer

Accuracy = 95.5
> 

![](/home/om/Desktop/SGD1.png)

Loss = 0.18

 ![](/home/om/Desktop/SGD2.png)

**FScore**
> from sklearn.metrics import classification_report

> print(classification_report(y_test, preds))

              
              precision    recall  f1-score   support

           0       0.75      0.99      0.85       980
           1       0.99      1.00      0.99      1135
           2       0.98      0.99      0.99      1032
           3       0.98      0.99      0.99      1010
           4       0.99      0.81      0.89       982
           5       0.99      0.98      0.99       892
           6       0.99      0.98      0.99       958
           7       0.99      0.95      0.97      1028
           8       0.99      0.99      0.99       974
           9       0.98      0.87      0.92      1009

    accuracy                           0.96     10000
    macro avg       0.96      0.96      \0.96     10000
    weighted avg    0.96      0.96     0.96     10000

**Confusion Matrix**
> from sklearn.metrics import confusion_matrix 

> confusion_matrix(y_test, preds)

array([[ 975,    1,    1,    1,    0,    0,    0,    1,    1,    0],
       
       [   0, 1131,    2,    1,    0,    0,    0,    0,    1,    0],
       [   2,    1, 1023,    1,    1,    0,    0,    3,    1,    0],
       [   0,    0,    1, 1004,    0,    1,    0,    1,    2,    1],
       [ 172,    0,    1,    3,  793,    0,    4,    0,    0,    9],
       [   6,    0,    0,    5,    1,  874,    2,    1,    2,    1],
       [   8,    2,    2,    1,    0,    1,  943,    0,    1,    0],
       [  30,    3,    9,    4,    2,    0,    1,  972,    1,    6],
       [   1,    0,    2,    2,    1,    2,    0,    2,  961,    3],
       [ 111,    1,    1,    0,    7,    2,    1,    5,    0,  881]])

## Adam

Accuracy = 98.15

  ![](/home/om/Desktop/ad1.png)

Loss = 0.69

![](/home/om/Desktop/ad2.png)

**FScore**
> from sklearn.metrics import classification_report

> print(classification_report(y_test, preds))

            precision    recall  f1-score   support

           0       0.99      0.99      0.99       980
           1       0.95      0.99      0.97      1135
           2       0.98      0.95      0.96      1032
           3       0.97      1.00      0.98      1010
           4       0.99      0.99      0.99       982
           5       0.99      0.98      0.98       892
           6       0.99      0.97      0.98       958
           7       0.99      0.98      0.99      1028
           8       0.99      0.99      0.99       974
           9       0.99      0.97      0.98      1009

    accuracy                           0.98     10000
    macro avg       0.98      0.98      0.98     10000
   weighted avg       0.98      0.98      0.98     10000

**Confusion Matrix**
> from sklearn.metrics import confusion_matrix 

> confusion_matrix(y_test, preds)

array([
   973,    3,    1,    1,    1,    0,    0,    0,    1,    0],
       
       [   0, 1126,    1,    2,    1,    0,    0,    2,    2,    1],
       [   2,   43,  980,    0,    0,    0,    0,    5,    2,    0],
       [   0,    1,    0, 1007,    0,    1,    0,    1,    0,    0],
       [   1,    1,    2,    1,  968,    0,    4,    1,    1,    3],
       [   0,    0,    0,   16,    0,  872,    1,    0,    2,    1],
       [   7,    4,    3,    2,    1,    5,  932,    0,    4,    0],
       [   2,    6,    9,    2,    0,    0,    0, 1009,    0,    0],
       [   1,    0,    4,    0,    0,    0,    0,    1,  965,    3],
       [   0,    1,    2,   11,    8,    1,    0,    1,    2,  983]])


